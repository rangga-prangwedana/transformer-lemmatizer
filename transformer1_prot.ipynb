{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "26zjgd_vaziH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '1'"
      ],
      "metadata": {
        "id": "myX6oCHRajp5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "pboVXHXgHfmQ",
        "outputId": "1ad092a7-edec-4f0e-a9ee-a2622ea79715"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/mnt/data/Flowchart_Alur_Penelitian.png'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification, AdamW, get_linear_schedule_with_warmup\n",
        "from datasets import load_dataset\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import datasets\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn as nn\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define model and tokenizer\n",
        "model_name = \"bert-base-multilingual-cased\"  # Or any other suitable model\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Define labels (Universal Dependencies POS tags)\n",
        "ud_tags = ['ADJ', 'ADP', 'ADV', 'AUX', 'CCONJ', 'DET', 'INTJ', 'NOUN', 'NUM', 'PART', 'PRON', 'PROPN', 'PUNCT', 'SCONJ', 'SYM', 'VERB', 'X']\n",
        "tag2id = {tag: id for id, tag in enumerate(ud_tags)}\n",
        "id2tag = {id: tag for tag, id in tag2id.items()}"
      ],
      "metadata": {
        "id": "v5Cf0fgFaG6Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def conllu_to_dataframe(file_path):\n",
        "    sentences = []\n",
        "    sentence = []\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if not line or line.startswith(\"#\"):\n",
        "                if sentence:\n",
        "                    sentences.append(sentence)\n",
        "                    sentence = []\n",
        "                continue\n",
        "            parts = line.split(\"\\t\")\n",
        "            if len(parts) >= 10:\n",
        "                sentence.append(parts)\n",
        "    df = pd.DataFrame(sentences, columns=['ID', 'FORM', 'LEMMA', 'UPOS', 'XPOS', 'FEATS', 'HEAD', 'DEPREL', 'DEPS', 'MISC'])\n",
        "    return df\n",
        "\n",
        "def tokenize_and_align_labels(examples):\n",
        "    tokenized_inputs = tokenizer(examples[\"FORM\"], truncation=True, is_split_into_words=True)\n",
        "\n",
        "    labels = []\n",
        "    for i, labels_in_example in enumerate(examples[\"UPOS\"]):\n",
        "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
        "        previous_word_idx = None\n",
        "        label_ids = []\n",
        "        for word_idx in word_ids:\n",
        "            if word_idx is None:\n",
        "                label_ids.append(-100)\n",
        "            elif word_idx != previous_word_idx:\n",
        "                label_ids.append(tag2id[labels_in_example[word_idx]])\n",
        "            else:\n",
        "                label_ids.append(-100)\n",
        "            previous_word_idx = word_idx\n",
        "        labels.append(label_ids)\n",
        "\n",
        "    tokenized_inputs[\"labels\"] = labels\n",
        "    return tokenized_inputs"
      ],
      "metadata": {
        "id": "Em7lzv1NaLss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data (replace with your file paths)\n",
        "train_file = \"train01.conllu\" # example file\n",
        "test_file = \"01test.conllu\" # example file\n",
        "\n",
        "train_df = conllu_to_dataframe(train_file)\n",
        "test_df = conllu_to_dataframe(test_file)\n",
        "\n",
        "train_dataset = datasets.Dataset.from_pandas(train_df)\n",
        "test_dataset = datasets.Dataset.from_pandas(test_df)\n"
      ],
      "metadata": {
        "id": "O-zK57LgaU3m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess the datasets\n",
        "tokenized_train_dataset = train_dataset.map(tokenize_and_align_labels, batched=True, remove_columns=train_dataset.column_names)\n",
        "tokenized_test_dataset = test_dataset.map(tokenize_and_align_labels, batched=True, remove_columns=test_dataset.column_names)\n",
        "\n",
        "# Training Parameters (Adjust as needed!)\n",
        "batch_size = 16  # Reduced for demonstration purposes\n",
        "learning_rate = 5e-5  # Standard value for fine-tuning\n",
        "beta_2 = 0.98\n",
        "dropout_rate = 0.3 # not used directly, handled by the pretrained model.\n",
        "label_smoothing = 0.6\n",
        "num_epochs = 3"
      ],
      "metadata": {
        "id": "YqV6cmVIaaIu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DataLoaders\n",
        "train_dataloader = DataLoader(tokenized_train_dataset, batch_size=batch_size, shuffle=True)\n",
        "eval_dataloader = DataLoader(tokenized_test_dataset, batch_size=batch_size)\n",
        "\n",
        "# Initialize the model\n",
        "model = AutoModelForTokenClassification.from_pretrained(model_name, num_labels=len(ud_tags))\n",
        "\n",
        "# Optimizer and Scheduler\n",
        "optimizer = AdamW(model.parameters(), lr=learning_rate, betas=(0.9, beta_2))\n",
        "num_training_steps = num_epochs * len(train_dataloader)\n",
        "lr_scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n",
        "\n"
      ],
      "metadata": {
        "id": "PvPIZHy1ZhOR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss Function with Label Smoothing\n",
        "loss_fct = nn.CrossEntropyLoss(label_smoothing=label_smoothing, ignore_index=-100)\n",
        "\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "5P9LlvWlZyeB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for batch in tqdm(train_dataloader, desc=f\"Epoch {epoch+1}\"):\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        outputs = model(**batch)\n",
        "        logits = outputs.logits\n",
        "        loss = loss_fct(logits.view(-1, model.num_labels), batch[\"labels\"].view(-1))\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "        lr_scheduler.step()\n",
        "        optimizer.zero_grad()"
      ],
      "metadata": {
        "id": "o4Qo8L4EZliZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation with Accuracy\n",
        "model.eval()\n",
        "total_correct = 0\n",
        "total_predictions = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        outputs = model(**batch)\n",
        "        logits = outputs.logits\n",
        "        predictions = torch.argmax(logits, dim=-1)\n",
        "\n",
        "        for i in range(predictions.shape[0]):\n",
        "            for j in range(predictions.shape[1]):\n",
        "                if batch[\"labels\"][i, j] != -100:\n",
        "                    total_predictions += 1\n",
        "                    if predictions[i, j] == batch[\"labels\"][i, j]:\n",
        "                        total_correct += 1\n",
        "\n",
        "accuracy = total_correct / total_predictions if total_predictions > 0 else 0\n",
        "print(f\"Evaluation Accuracy: {accuracy}\")\n",
        "\n",
        "print(\"Training and Evaluation Complete!\")"
      ],
      "metadata": {
        "id": "_hMTVeZEZkiB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XL3agpSoH35s"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}